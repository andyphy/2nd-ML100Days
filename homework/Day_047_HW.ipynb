{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "了解如何使用 Sklearn 中的 hyper-parameter search 找出最佳的超參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Reference]\n",
    "- [劍橋實驗室教你如何調參數](https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/)\n",
    "- [教你使用 Python 調整隨機森林參數](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n",
    "- 隨機搜尋通常都能獲得更更佳的結果，[Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)\n",
    "- [Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業\n",
    "請使用不同的資料集，並使用 hyper-parameter search 的方式，看能不能找出最佳的超參數組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston Housing Price Feature Array Shape: (506, 13)\n",
      "Boston Housing Price Target Array Shape: (506,)\n"
     ]
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "print('Boston Housing Price Feature Array Shape:', x.shape)\n",
    "print('Boston Housing Price Target Array Shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CRIM     ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['CRIM ', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO'\n",
    "        , 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.DataFrame(np.concatenate((x, y.reshape(len(y), 1)), axis=1), columns=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [scikit-learn 梯度提升树(GBDT)调参小结](https://www.cnblogs.com/pinard/p/6143927.html)\n",
    "### [sklearn.ensemble.GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "- loss : {‘ls’, ‘lad’, ‘huber’, ‘quantile’}, optional (default=’ls’)\n",
    "loss function to be optimized. ‘ls’ refers to least squares regression. ‘lad’ (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use alpha to specify the quantile).\n",
    "- learning_rate : float, optional (default=0.1):\n",
    "每個弱學習器的權重縮減係數，也稱作步長。小的learning_rate意味着需要更多的弱學習器的迭代次數。通常用步長和迭代最大次數一起来决定算法的擬合效果。所以n_estimators和learning_rate要一起調參。\n",
    "- n_estimators : int (default=100):\n",
    "弱學習器的最大迭代次數，或者說最大的弱學習器的個數。一般來說n_estimators太小，容易欠擬合，n_estimators太大，又容易過擬合，一般選擇一個適中的數值。默認是100。在實際調參的過程中，我們常常將n_estimators和下面介紹的參數learning_rate一起考慮。\n",
    "- alpha：The alpha-quantile of the huber loss function and the quantile loss function. Only if loss='huber' or loss='quantile'.\n",
    "這個參數只有GradientBoostingRegressor有，當我們使用Huber損失\"huber\"和分位數損失“quantile”時，需要指定分位數的值。默認是0.9，如果噪音點較多，可以適當降低這個分位數的值。\n",
    "\n",
    "- max_depth : integer, optional (default=3)\n",
    "maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 8.01\n"
     ]
    }
   ],
   "source": [
    "# 用 Gradient Boosting regressor \n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=2) \n",
    "# 建立模型\n",
    "reg = GradientBoostingRegressor(random_state=1)\n",
    "# 訓練模型\n",
    "reg.fit(x_train, y_train)\n",
    "# 預測測試集\n",
    "y_pred = reg.predict(x_test)\n",
    "# 預測值與實際值的差距，使用 MSE\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV  \n",
    "#### Reference\n",
    "- Official API:[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- [sklearn學習8-GridSearchCV(自動調參)](https://www.itread01.com/content/1529133726.html)\n",
    "- [機器學習_ML_GridSearchCV_網格搜尋](https://martychen920.blogspot.com/2017/09/ml-gridsearchcv.html)\n",
    "#### Parameter Introduction\n",
    "- estimator:使用的分類器\n",
    "- param_grid: list 或是 dict 做最佳化的參數\n",
    "- [scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)=None: None的時候即使用estimator的預設score\n",
    "- n_jobs=None: n_jobs=-1 會使用全部 cpu 平行運算, 如果設置-1或>1的話，在數據量過大的情況下就可能會因為記憶體不足error\n",
    "- iid=’warn’: 默認為各個樣本fold概率分布一致，誤差估計為所有樣本之和，而非各個fold的平均。\n",
    "- refit=True: 設置為true的話，會在最後取得最佳參數之後再以該參數做fit一次全部的資料。\n",
    "- cv=’warn’: 交叉驗證，default 3-fold cross validation。將資料分n份, 預設情況下是以KFold來處理的。除非你是多分類才會以StratifiedKFold。\n",
    "- verbose=0: 設置為0即訓練過程不會顯示，為1的話偶爾顯示，大於1的時候對每個子模型都輸出。\n",
    "- pre_dispatch=‘2*n_jobs’:指定總共分發的並行任務數。當n_jobs大於1時，數據將在每個運行點進行復制，這可能導致OOM(out of memory)\n",
    "- error_score=’raise-deprecating’: Value to assign to the score if an error occurs in estimator fitting.\n",
    "- return_train_score=False:如果設置False就不會回傳結果!即屬性cv_results_ 就不會有分數的值了。\n",
    "#### Attributes\n",
    "- cv_results_: 這邊記錄著你的所有參數的狀況，是個dict(字典)形態。可以直接丟給pandas\n",
    "- grid.fit( train_x, train_y )：運行網格搜索\n",
    "- grid_score_：給出不同參數情況下的評價結果\n",
    "- best_params_：取得最最佳結果的參數組合\n",
    "- best_estimator_: 最佳分類器，如果refit是False就沒有效果!\n",
    "- best_score_：最佳分類器的平均驗證得分\n",
    "- best_index_: 在cv_results_中的最佳參數的索引值\n",
    "- n_splits_: cv的保存!\n",
    "#### Methods\n",
    "- decision_function(X): Call decision_function on the estimator with the best found params. 只有在refit=True跟分類器可以實作的時候才有效果。以最佳參數執行!\n",
    "- fit(X[, y, groups]): 適合、訓練模型(開始用設置的參數開始暴力測試)\n",
    "- get_params([deep]): 取得模型參數\n",
    "- inverse_transform(Xt): Call inverse_transform on the estimator with the best found params. 只有在refit=True跟分類器可以實作的時候才有效果。以最佳參數執行!各演算法的inverse_transform意義不同，可參閱SKlearn的api說明!\n",
    "- predict(X)Call predict on the estimator with the best found parameters. 用最佳參數來做預測，特別貼出來說明是取最佳參數來做預測!\n",
    "- predict_log_proba(X): 回傳類別概率對數(機率、或然率)…一樣是以最佳參數來執行\n",
    "- predict_proba(X): 回傳類別概率(機率、或然率)…一樣是以最佳參數來執行\n",
    "- score(X[, y]):看你的上面參數SCROING設置來回傳，上面是NONE就以選擇的分類器的SCORE為預設\n",
    "- set_params(**params): 設置模型參數\n",
    "- transform(X): Call transform on the estimator with the best found parameters.以最佳參數來做轉換，但一樣要分類器可以實作!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StayFoolish\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:   16.4s finished\n",
      "C:\\Users\\StayFoolish\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 設定要訓練的超參數組合\n",
    "random_state=[1]\n",
    "loss = ['ls', 'lad', 'huber', 'quantile']\n",
    "n_estimators = [50, 100, 125, 150]\n",
    "max_depth = [1, 3, 5]\n",
    "param_grid = dict(random_state=random_state, \n",
    "                  loss=loss, \n",
    "                  n_estimators=n_estimators,\n",
    "                  max_depth=max_depth)\n",
    "\n",
    "## 建立搜尋物件，放入模型及參數組合字典 (n_jobs=-1 會使用全部 cpu 平行運算)\n",
    "grid_search = GridSearchCV(reg, param_grid, scoring=\"neg_mean_squared_error\", n_jobs=-1, verbose=1)\n",
    "\n",
    "# 開始搜尋最佳參數\n",
    "grid_result = grid_search.fit(x_train, y_train)\n",
    "\n",
    "# 預設會跑 3-fold cross-validadtio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: -8.732962 using {'loss': 'ls', 'max_depth': 3, 'n_estimators': 150, 'random_state': 1}\n"
     ]
    }
   ],
   "source": [
    "# 印出最佳結果與最佳參數\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 8.00\n"
     ]
    }
   ],
   "source": [
    "# 使用最佳參數重新建立模型\n",
    "reg_bestparam = GradientBoostingRegressor(random_state=grid_result.best_params_['random_state'],\n",
    "                                          loss=grid_result.best_params_['loss'],\n",
    "                                          max_depth=grid_result.best_params_['max_depth'],\n",
    "                                          n_estimators=grid_result.best_params_['n_estimators'])\n",
    "\n",
    "# 訓練模型\n",
    "reg_bestparam.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = reg_bestparam.predict(x_test)\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
